{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "military-honolulu",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "from pathlib import Path\n",
    "PROJ_HOME = '/Users/skeem396/Projects/'\n",
    "sys.path.append(PROJ_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deluxe-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import top_k_top_p_filtering\n",
    "import torch\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "developmental-organization",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5901971d7a48919f896ced719faafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.46k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154feb795e7341bbade1f2805f51459e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/10.7G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d56b8d025a684df29142f9ad7cbfd37f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae469d19b88425ea7aee32d96a12db6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "367163ca1af14d3bbb1863757f30ca39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7cfec64868ad4f56addd5971cc7005ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/200 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "mineral-arthritis",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'FreeWheel is the original, largest, and most popular choice for bike rentals in Houston. Our company has been providing quality bike rentals and repair services for over three decades. In 2010, Over the Road Bike provided a new name and a new beginning,'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"FreeWheel is\", do_sample=True, min_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "endangered-salon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'Comcast is expanding its cable bundle with new digital tiers for new subscribers, and will roll out more deals as part of an expansion of its current lineup in the coming months.\\n\\nThe company will offer customers in the US starting today for the first'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generator(\"Comcast is\", do_sample=True, min_length=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "circular-america",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "turkish-banking",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \"\"\"How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "checked-foundation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [2437, 867, 517, 15102, 761, 284, 307, 2923, 287, 1502, 329, 262, 1705, 12527, 11, 2592, 8661, 3392, 11, 284, 892, 326, 356, 447, 247, 260, 12733, 286, 257, 1621, 30, 2619], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "refined-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_next(phrase, length=10):\n",
    "    for i in range(length):\n",
    "        inputs = tokenizer.encode(phrase, return_tensors='pt')\n",
    "\n",
    "        # Get logits from last layer\n",
    "        last_layer_logits = model(inputs).logits[:, -1, :]\n",
    "        top_logits = top_k_top_p_filtering(last_layer_logits, top_k=100, top_p=1.0)\n",
    "\n",
    "        # Softmax the logits into probabilities\n",
    "        probabilities = F.softmax(top_logits, dim=-1)\n",
    "\n",
    "        # Generate next token\n",
    "        generated_next_token = torch.multinomial(probabilities, num_samples=1)\n",
    "        generated = torch.cat([inputs, generated_next_token], dim=-1)\n",
    "\n",
    "        tok_id = generated.tolist()[0]\n",
    "        result_string = tokenizer.decode(tok_id)\n",
    "        print(result_string)\n",
    "        phrase = result_string\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-assault",
   "metadata": {},
   "source": [
    "GPT-Neo was trained on the Pile, a dataset known to contain profanity, lewd, and otherwise abrasive language. Depending on your usecase GPT-Neo may produce socially unacceptable text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "beautiful-robert",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are already\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are already constantly\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are already constantly being\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are already constantly being told\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are already constantly being told not\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are already constantly being told not to\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are already constantly being told not to call\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are already constantly being told not to call out\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Black people are already constantly being told not to call out'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n_next(test_text, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "checked-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text2 = \"\"\"How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian\"\"\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "still-appreciation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die at\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die at a\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die at a higher\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die at a higher rate\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die at a higher rate than\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die at a higher rate than any\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die at a higher rate than any other\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die at a higher rate than any other group\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story? Asian Americans die at a higher rate than any other group'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n_next(test_text2, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "wooden-sierra",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this,\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this, which\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this, which only\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this, which only gets\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this, which only gets worse\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this, which only gets worse after\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this, which only gets worse after it\n",
      "How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this, which only gets worse after it�\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story like this, which only gets worse after it�'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text3 = \"\"\"How many more blacks need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story\"\"\" \n",
    "generate_n_next(test_text3, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "varying-effectiveness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "\n",
      "\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "\n",
      "\n",
      "\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This piece\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This piece first\n",
      "How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "This piece first appeared\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story?\\n\\n\\n\\n\\nThis piece first appeared'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text4 = \"\"\"How many more Asians need to be killed in order for the news outlets, especially mainstream ones, to think that we’re worthy of a story\"\"\"\n",
    "generate_n_next(test_text4, length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-packet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "nlp_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
